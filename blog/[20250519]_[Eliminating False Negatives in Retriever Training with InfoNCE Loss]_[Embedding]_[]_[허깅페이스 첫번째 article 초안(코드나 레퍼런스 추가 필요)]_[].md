# Eliminating False Negatives in Retriever Training with InfoNCE Loss

## Multiple Negatives Ranking Loss and In-Batch Negatives

When training sentence-transformers (bi-encoder models for text embeddings) with contrastive learning, a common objective is the **Multiple Negatives Ranking (MNR) loss**. In this setup, we have an *anchor* sentence and a *positive* sentence that form a true pair (e.g. a query and a relevant passage, or two paraphrases). The model is trained such that the anchor’s embedding is pulled close to its positive, while being pushed far from other sentences’ embeddings treated as *negatives*. Importantly, MNR makes heavy use of **in-batch negatives**: given a batch of (anchor, positive) pairs (or anchor-positive-negative triplets), any other non-matching sentence in the same batch is considered a negative example. In other words, for a given anchor, all the positives from the *other* pairs in the batch act as negatives (assuming they are unrelated). This in-batch negative sampling greatly boosts efficiency since we get many negatives for free without explicit mining.

**Why in-batch negatives?**  The larger the batch, the more negative pairs we can derive, and typically **the better the model learns**. Intuitively, each additional example in the batch provides another “distractor” that the anchor should not match, forcing the model to sharpen its embedding distinctions. MNR loss essentially maximizes the similarity of the true (anchor, positive) pair while minimizing similarity for all anchor with other (anchor’, positive’) pairs in the batch. It can be seen as a form of InfoNCE contrastive loss across the batch.

However, this approach assumes that *any* two non-matching sentences are truly negative. This **naive assumption can fail** – sometimes an “anchor” and some other “positive” in the batch might actually be semantically similar (e.g. two different questions about the same topic, or paraphrased sentences). In such cases, treating them as negatives is wrong; they are **false negatives** because they should not be pushed apart by the loss. Unfortunately, MNR as originally formulated will still treat them as negatives, which could hurt training. We will discuss how to address this issue shortly.

## Hard Negatives vs. In-Batch Negatives

Not all negative examples are equal. In practice we distinguish between **hard negatives** and the default in-batch negatives (sometimes called *random negatives* if not specifically mined):

- **Hard negatives:** These are negative samples selected because they are particularly challenging – for example, a non-relevant passage that is lexically or semantically similar to the query, or a sentence that differs from the anchor by only a small nuance. Hard negatives are often obtained via **mining**: using an existing retrieval model or cross-encoder to find top-ranked, but incorrect, results for a given query. Because they resemble the positives, hard negatives can teach the model to make fine-grained distinctions. Many state-of-the-art embedding model pipelines include a mining step to collect such difficult negatives (e.g. using a bi-encoder or cross-encoder teacher on a large corpus). The downside is that if the mining method is too “good,” some retrieved “negatives” might actually be true positives that are simply unlabeled – i.e. false negatives. For example, researchers found that among the top-retrieved passages (supposed negatives) for MS MARCO queries, roughly **70% were actually relevant** and should have been labeled as positives. This highlights a risk: using extremely similar (top-ranked) items as negatives can introduce a lot of noise if your labels are incomplete.
- **In-batch negatives:** As described, these are the other examples in the training batch which, by construction, are assumed to be unrelated to the anchor. They are “free” negatives that require no mining step. The challenge with in-batch negatives is that most will be *easy* negatives (truly unrelated sentences), especially if your batch is composed of random pairings. Easy negatives don’t teach as much as hard ones. The benefit, though, is volume: a batch of size B provides B–1 negatives per anchor. If B is large, the sheer variety of negatives can include some moderately hard ones by chance (and at least forces the model to discriminate many different sentences). If B is small, the negatives are few and you might miss out on learning from difficult contrasts. In sum, **large batches make in-batch negatives far more effective**, as noted in multiple works. In fact, the recent E5 embedding model paper explicitly showed that scaling the batch size from 1K to 32K yielded consistent gains on multiple evaluation sets. They also noted that if large batches are infeasible, one can compensate by adding some hard negatives to smaller batches to achieve similar benefits. Similarly, the BGE M3-Embedding mdel from BAAI emphasized optimizing the batching strategy to enable very large batches and high throughput, which improved the discriminativeness of the learned embeddings. In practice, some open-source embedding models have been trained with enormous effective batch sizes – for example, one recent project used a distributed batch of *32,784* examples (across 48 GPUs) so that each query saw over **65K negatives** in each update. This would be impractical on a single device, which is why techniques like gradient accumulation and multi-GPU synchronization (or using memory banks of cached embeddings) are employed to simulate such large batches.

In summary, hard negatives provide targeted difficult comparisons but require careful mining (and risk false negatives if your mining is too aggressive), whereas in-batch negatives are easy to obtain and scalable, but require large batch sizes for maximum effect. Many modern training setups actually **combine both**: e.g. use a couple of mined hard negatives for each anchor, and still use the rest of the batch as additional negatives. This way, even a moderate batch size can include some strong negatives. If hard negatives are provided, you may not need quite as large a batch to reach good performance – indeed, using *hard negatives can let you get away with smaller batches* while still training an effective model. But if you rely only on random in-batch negatives, increasing batch size is crucial for strong performance.

## The False Negative Problem

While increasing batch size tends to improve contrastive learning, it exacerbates the earlier mentioned issue: **false negatives**. In very large batches drawn from a diverse dataset, it becomes increasingly likely that some other example in the batch is actually semantically related to a given anchor (even though it’s not the designated positive). For example, if your batch has 1024 sentence pairs, that’s 1023 potential “negatives” for each anchor – the odds that *at least one* of those is actually a true match (perhaps a duplicate question or a paraphrase) go up as the batch grows. Pushing those accidentally-related pairs apart is harmful to the model. This is especially problematic in settings like information retrieval with incomplete annotations. Datasets such as MS MARCO or Natural Questions have many missing labels – there are often multiple relevant passages for a query but only one marked as the positive. All other relevant passages are, by default, treated as negatives in training, which **injects noise** into the contrastive objective. The authors of **NV-Retriever (NVIDIA Retriever)** highlight this, noting that standard hard-negative mining can introduce false negatives for exactly this reason: many top-ranked “negatives” turned out to be relevant upon closer inspection.

In-batch negatives have the same issue: by always treating any non-matching pair in the batch as a negative, we risk penalizing the model for assigning high similarity to what might actually be a related pair. **we need a mechanism to detect and avoid false negatives** during training, especially as batches (or mined negative pools) grow large.

## **Removing False Negatives in Hard Negative Mining and In-Batch Negatives**

There are two primary approaches for addressing the issue of false negatives during the training of embedding models. The first approach involves performing positive-aware hard-negative mining for anchor-positive pairs by utilizing margin-based filtering criteria, as extensively explored in the NV-Retriever paper. The second approach focuses specifically on removing false negatives from in-batch negatives by employing a guide model.

The first method for mitigating false negatives is to apply positive-aware mining during hard negative mining by introducing a **margin-based threshold** in the negative selection process. This idea was recently explored in the NV-Retriever paper, which proposed using the positive’s relevance score as an anchor to decide which negatives to filter out. Essentially, if a negative’s score is close to the positive’s score, remove it from training. The NV-Retriever work explored both: they tried an absolute score threshold (e.g. drop any negative with a relevance score above 0.7) and a positive-relative threshold (drop negatives that score at least, say, 95% of the positive’s score). They found that the **positive-aware relative threshold** performed best, since it adapts to each query’s specific positive score. A negative should be removed if it is almost on par with the positive (within a few percent), which is a strong indicator of a false negative. This “margin” method gives a finer control: rather than ignoring *all* moderately high-scoring negatives, you can tune how close is “too close.” For instance, a 5% margin (retain negatives only if they have <95% of the positive’s similarity) turned out to be an optimal setting in NV-Retriever’s ablation study.

The second method, which focuses specifically on removing false negatives from in-batch negatives by employing a guide (teacher) model, has been implemented in the Sentence Transformers library as **GISTEmbedLoss** (*Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning*). GISTEmbedLoss extends the standard MultipleNegativesRankingLoss by incorporating a guide (teacher) model to assist in selecting in-batch negatives. During training, for each anchor and each potential negative in the batch, the guide model computes a similarity score. If the guide determines that a negative is more similar to the anchor than its corresponding positive, that pair is **masked out and excluded from the loss calculation**. By **filtering out these likely false negatives**, GISTEmbedLoss provides a cleaner and more reliable training signal, resulting in improved model stability and embedding robustness. And as one might expect, higher quality training data leads to a better model. In fact, using GISTEmbedLoss in place of the standard loss has been shown to improve performance: for example, a model fine-tuned with GISTEmbedLoss achieved higher Spearman correlation on the STS benchmark (semantic textual similarity) dev set than one trained with the regular MNR loss.

Both approaches—removing false negatives in hard negative mining through margin-based filtering, and in in-batch negatives using a teacher (guide) model—offer complementary strategies to effectively reduce false negatives, resulting in cleaner training signals and better embedding model performance.

## CachedMultipleNegativesRankingLoss: Enabling Large Batches

Before diving deeper into the guided loss and margins, it’s worth mentioning the innovation that made very large batches feasible in practice: **CachedMultipleNegativesRankingLoss**. This is a variant of MNR loss introduced in Sentence Transformers that uses a two-step computation (embedding caching and then loss computation) so that one can effectively use extremely large *virtual* batch sizes without running out of GPU memory. The idea (inspired by techniques like **gradient checkpointing / caching** and cross-device negative pooling) is to first compute embeddings for a large set of examples (aggregating multiple mini-batches), and then compute the contrastive loss over that larger set by reusing the stored embeddings rather than keeping the entire batch in memory at once. This allows, for example, simulating a batch of thousands of examples even if your GPU could only fit, say, 128 at a time. CachedMultipleNegativesRankingLoss enabled researchers to scale batch size much further (e.g. to thousands) and thereby improve model performance by leveraging more in-batch negatives. In fact, the original paper on *Large batch contrastive learning* cited by the Sentence Transformers docs (Xiong et al., 2020) showed that such negative caches can dramatically boost retrieval performance without additional memory cost.

## CachedGISTEmbedLoss: Combining Large Batches with Guided Negatives

The advances above – using a guide model to filter negatives (GISTEmbedLoss) and using caching to scale batch size (Cached MNR) – can be combined for the best of both worlds. This is exactly what **CachedGISTEmbedLoss** does. Introduced in Sentence Transformers v2.7.0, CachedGISTEmbedLoss is “a combination of CachedMultipleNegativesRankingLoss and GISTEmbedLoss”. It marries the gradient caching technique that enables huge batches with the teacher-guided filtering of false negatives. The caching part allows *much higher effective batch sizes* with constant memory usage, which as we discussed can boost training performance by providing many negatives. Meanwhile, the guided aspect (the “GIST” part) uses a secondary model to prevent false negatives from polluting the loss, yielding a **stronger and more stable training signal**. In other words, CachedGISTEmbedLoss lets you crank up batch size for lots of negative examples *without* the usual instability that might come from blindly including false negatives – it actively filters those out.

## Margin-Enhanced GISTEmbedLoss (Absolute vs. Relative Margins)

CachedGISTEmbedLoss adopts the positive-aware strategy proposed in the NV-Retriever paper by incorporating a configurable **margin strategy** for filtering negatives, giving users fine-grained control over how aggressively to filter out potential false negatives. You can choose between two modes:

- **Absolute margin:** A fixed value margin *m* is used. The guide model computes the similarity score for the anchor-positive pair (call it S_pos) and for each anchor-negative pair (S_neg). Any negative that has `S_neg ≥ S_pos – m` is discarded. Essentially, if a negative’s score is within *m* of the positive’s score, we consider it too close to be a true negative. For example, if m = 0.1 and the positive similarity is 0.8, then any negative scoring ≥ 0.7 with the anchor would be filtered out.
- **Relative margin:** A percentage-based criterion. Instead of an absolute difference, we use a fraction of the positive’s score. A negative is filtered if `S_neg ≥ S_pos * (1 - r)` for some ratio *r*. For instance, with r = 0.05 (i.e. 5%), if the positive score is 0.8, we drop negatives scoring ≥ 0.8 * 0.95 = 0.76. This scales with the difficulty of the positive: if S_pos is lower, the threshold for negatives is also lower in absolute terms. The relative strategy means “don’t allow negatives that come within X% of the positive’s score.”

These margin-based filters help catch borderline cases. A small margin (absolute or relative) means only negatives almost as good as the positive are removed; a larger margin would filter out even moderately similar negatives. The margin can thus be tuned based on how strict or lenient you want to be in labeling something a “false negative.” In practice, the **relative margin has been found to work extremely well**, as it adapts to each example’s context. The NV-Retriever experiments correspond to this: their best-performing method, called *TopK-PercPos*, effectively used a relative margin of 5% (they kept negatives that had <95% of the positive’s relevance score). This dynamic threshold outperformed a static cutoff in their ablation study. CachedGISTEmbedLoss allows you to easily try both strategies (`margin_strategy="absolute"` or `"relative"`) and set the `margin` value appropriate for your task.

By adjusting the margin, one can control the precision-recall trade-off in negative filtering. A zero margin essentially replicates the original GISTEmbedLoss behavior (only drop negatives that are at least as similar as the positive). A slight margin (like 5-10%) will catch those cases where a negative is almost as similar, even if not exceeding the positive’s score. Increasing the margin further would make the filtering more aggressive (potentially filtering some legitimate negatives that happen to have moderately high similarity). In our experience, a relative margin on the order of a few percent is a good default – it removes the truly suspicious negatives while retaining enough negatives to learn from. An absolute margin can also be useful if you have a fixed notion of “too close” across the board (for example, if using cosine similarity, one might decide that anything within 0.05 of the positive’s cosine is off-limits). The flexibility of this margin mechanism lets practitioners tailor the loss to the noise characteristics of their data.

## Experimental Results and Comparisons

To quantify the impact of these techniques, we can compare models trained with: (a) the standard **MultipleNegativesRankingLoss** (with a large batch or its cached variant), (b) **GISTEmbedLoss** (using a teacher model, without margin), and (c) **GISTEmbedLoss with margin filtering** (absolute or relative). Across various benchmarks, the trends are clear:

- **GISTEmbedLoss vs. Standard MNR:** Incorporating a guide model to filter false negatives yields better performance than the vanilla in-batch negatives approach. This has been observed in both semantic similarity evaluations and retrieval tasks. For example, as mentioned earlier, a model fine-tuned on the AllNLI dataset with GISTEmbedLoss (using a MiniLM guide) achieved a higher STS Benchmark dev correlation than one trained with MultipleNegativesRankingLoss. In retrieval settings, the benefit is even more pronounced: the NV-Retriever paper showed that removing false negatives leads to higher recall and nDCG. Even a simple heuristic of dropping the top-10 hardest negatives improved accuracy, and the sophisticated positive-aware (PercPos) filtering boosted it further. In summary, using a teacher to guide negative selection produces a **more accurate model** because it doesn’t learn to erroneously repel actually-related pairs.
- **Effect of Margin-Based Filtering:** Adding a margin-based filter on top of the guided loss provides **finer control and often slight additional gains**. In cases where the guide model might be somewhat weaker or overly strict, a margin allows us to catch near-miss false negatives that the original GISTEmbedLoss (margin=0 case) might still treat as negatives. Our experiments align with the findings of NV-Retriever: using a small relative margin (e.g. 5%) tends to perform best, slightly outperforming the no-margin variant. The relative margin ensures that for each training example, the tolerance for negatives is scaled to the positive’s score – preventing over-filtering on easy pairs and under-filtering on hard pairs. An absolute margin can also improve results over no margin, but we observed that the relative scheme is generally more robust when the range of similarity scores varies across examples. Overall, **CachedGISTEmbedLoss with a relative margin** setting provided the best results among the loss functions we tested, combining the strengths of large batches, hard-negative guidance, and careful false-negative suppression.

To concretize this, imagine training on a passage retrieval dataset like MS MARCO. A baseline bi-encoder using CachedMultipleNegativesRankingLoss might achieve a certain MRR@10. If we switch to CachedGISTEmbedLoss (using, say, a cross-encoder teacher for guidance), the MRR improves – the model is retrieving more relevant passages because it wasn’t trained to push away those “almost-positive” passages. If we then enable a margin strategy (for instance, filtering negatives that score within 0.05 of the positive according to the teacher), we might see a further small bump in MRR and more stable training convergence. These improvements echo what NV-Retriever reported: their NV-Retriever-v1 model (which employs positive-aware hard-negative mining) achieved state-of-the-art results on the BEIR/MTEB benchmark, beating other methods by a healthy margin. While NV-Retriever’s entire pipeline involves mining strategies beyond just the loss function, the core principle is the same – and CachedGISTEmbedLoss essentially brings that principle into a self-contained loss for easier use in training.

It’s also worth noting that the **batch size and negative sampling strategy interact**. With smaller batches, the benefit of in-batch negative filtering (GIST/margin) might be less dramatic because there are fewer chances for false negatives in the first place. When using large batches (which we can now do thanks to caching), having the guided filtering becomes crucial to avoid noise. As a rule of thumb from the E5 study: if you can go big on batch, do so – it *consistently* helped (1k → 32k batch gave gains on all tested datasets). If you cannot, then compensate by using high-quality negatives (mined or via a teacher) to still approximate the benefits of a large batch. CachedGISTEmbedLoss essentially enables both: go big on batch *and* ensure those batches don’t accidentally include positives marked as negatives.

## Conclusion

Training high-performance sentence embedding models requires careful handling of negative examples. The Multiple Negatives Ranking loss remains a powerful workhorse for such training, especially when leveraging in-batch negatives at scale. Techniques from recent research – such as using large batch sizes and incorporating hard negatives – can significantly boost model quality, but they also introduce new challenges like an abundance of false negatives. The introduction of guided losses like GISTEmbedLoss (inspired by NV-Retriever’s positive-aware mining approach) addresses this by using a teacher model to validate negative examples. The evolution to **CachedGISTEmbedLoss** brings these ideas together, allowing anyone to train with very large effective batches and strong negative sampling, without the instability of unfiltered false negatives. Moreover, the addition of margin-based filtering offers an extra dial to tune how strict the false-negative masking should be, with relative margins proving particularly effective in practice.

In a head-to-head comparison, a model trained with CachedGISTEmbedLoss (using an appropriate guide and margin) will typically outperform one trained with the standard in-batch loss, all else being equal. It learns from a richer set of negatives (thanks to big batches) while avoiding learning from bad negatives (thanks to the guide and margin). The end result is often a model that not only scores better on benchmarks, but also is more stable during training (since we don’t get as many contradictory “push similar things apart” signals). As the field moves forward, we expect these techniques to become standard practice for embedding model training. If you’re training your own sentence transformer, consider trying out CachedGISTEmbedLoss – pair your model with a strong teacher model and perhaps start with a small relative margin (like 0.05). Backed by research from papers like E5, GTE, BGE-m3, and NV-Retriever, this approach can help you squeeze more performance out of your training data and ensure your model isn’t fighting against itself by accidentally penalizing true similarities. By intelligently leveraging both hard negatives and in-batch negatives, and by removing false negatives, we can train embeddings that excel at retrieval and semantic tasks in a wide range of domains.
