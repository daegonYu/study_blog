https://arxiv.org/pdf/2201.11903

# 읽고 느낀 점 (feat. LIMO, Deepseek)

1. LIMO나 이 CoT 논문이나 모델이 기존에 알고 있는 내용을 추론과정으로 이끌어 낸다는 컨셉은 같다. LIMO는 학습으로 이끌어내고 CoT 논문은 프롬프팅을 통해 이끌어낸다. 

2. SFT 학습으로도 CoT는 효과적이다. 특히 작은 모델일 때

  이유 : 
  
  - LIMO 논문도 SFT 학습으로 Reasoning 모델을 만들어냈고 딥시크-distillation 모델도 SFT 학습을 하였다. Reasoning 모델은 SFT 학습으로 이끌어낼 수 있으며 모델 사이즈가 작을 때 SFT 방법이 더 효과적이다. 마찬가지로 이 논문에서는 모델 사이즈가 작을 때 CoT 프롬프팅이 없을 때와 존재할 때의 성능 차이는 미미했다. 즉, 추론은 모델 사이즈가 어느정도 뒷받침 되어야 한다.
  - 모델이 추론을 이해하는 과정은 난이도가 있기 때문에 모델 사이즈가 작으면 정답을 알려주는 방식(SFT)로 학습하는 것이 효과적이고 모델 사이즈가 크다면 직접 추론 과정을 거쳐 학습하는 RL(강화학습) 방식으로 학습하는 것이 효과적이다.

3. 논리가 필요없는 태스크에는 CoT 하는 것이 오히려 성능을 악화시킬 수 있다. 모든 태스크에 CoT가 정답은 아니다.

---

## **🔹 Chain-of-Thought Prompting (CoT)의 개념**

### **1️⃣ 배경 및 문제 인식**

- 인간이 복잡한 문제(예: 수학 문제)를 해결할 때는 보통 **중간 추론 단계를 거쳐 최종 답을 도출**함.
    - 예:*"Jane이 엄마에게 2송이 꽃을 주면 10송이가 남고, 아빠에게 3송이를 주면 7송이가 남는다. 따라서 정답은 7."*
- 대형 언어 모델(LLM)도 이와 유사한 **추론 과정(Chain of Thought, CoT)**을 생성하도록 유도하면 성능이 향상될 것이라는 가설을 제시.
- 논문의 핵심 목표:**언어 모델이 단계적 사고(Reasoning)를 가능하게 만드는 방법을 제시하고, 이를 통해 모델의 성능을 향상시키는 것.**

---

### **2️⃣ CoT Prompting의 정의**

- **Few-shot Prompting 기법을 확장한 개념.**
- 기존 Few-shot Prompting과 달리, **입력(Input)과 출력(Output) 사이에 중간 추론 과정(Chain of Thought)을 포함**하는 방식.
- 이를 통해 LLM이 자연스럽게 논리적 사고 과정을 모방하도록 유도.
- 예제(Prompt) 형태: 입력, **추론 과정** (CoT), 답변

### **🔹 기존 Prompting vs. CoT Prompting 비교**

✅ **기존 Few-shot Prompting 예시** (단순 질의응답)

```
Q: 철수가 사과 3개를 가지고 있고, 2개를 더 샀습니다. 총 몇 개의 사과를 가지고 있나요?
A: 5

```

➡ 모델이 문제를 보고 한 번에 답을 예측해야 함.

✅ **CoT Prompting 예시** (추론 과정 포함)

```
Q: 철수가 사과 3개를 가지고 있고, 2개를 더 샀습니다. 총 몇 개의 사과를 가지고 있나요?
A: 철수는 처음에 3개의 사과를 가지고 있습니다.
    철수는 2개의 사과를 추가로 샀습니다.
    따라서, 철수는 총 3 + 2 = 5개의 사과를 가지고 있습니다.
    정답: 5

```

➡ 모델이 논리적 사고 과정을 따를 수 있도록 유도.

---

## **🔹 Chain-of-Thought Prompting의 특성**

논문에서는 CoT Prompting이 가진 **4가지 주요 장점**을 제시함.

1️⃣ **복잡한 문제를 단계적으로 해결 가능**

- 모델이 문제를 여러 개의 중간 단계로 나누어 해결하도록 함.
- 즉, **더 많은 계산량을 논리적 추론에 할당할 수 있음**.

2️⃣ **모델의 추론 과정을 해석 가능**

- 모델이 어떻게 답을 도출했는지를 확인할 수 있어 **디버깅과 오류 분석이 쉬움**.
- (단, 모델의 내부 계산 과정까지 완전히 설명하는 것은 여전히 어려움.)

3️⃣ **다양한 Reasoning Task에 적용 가능**

- 수학 문제, 상식적 추론(Commonsense Reasoning), 기호 논리(Symbolic Manipulation) 등 다양한 문제 유형에 효과적.
- 기본적으로 **사람이 언어로 해결할 수 있는 문제라면 CoT 적용이 가능**.

4️⃣ **대형 모델에서만 효과적으로 작동**

- **100B+ 모델 이상에서 CoT Prompting의 효과가 나타남**.
- 작은 모델에서는 CoT를 사용해도 **추론 과정이 논리적으로 일관되지 않거나 오류가 많음**.

---

## **🔹 실험 및 성능 평가**

논문에서는 **CoT Prompting의 효과를 실험적으로 검증**함.

### **1️⃣ Arithmetic Reasoning (산술 추론)**

- 수학 문제 풀이 데이터셋(GSM8K 등)에서 실험.
- **CoT Prompting을 사용한 540B 모델(PaLM 540B)이 기존 Fine-tuned 모델 수준의 성능을 달성**.
- **특히, GSM8K에서 SOTA 달성**.
    - 기존 GPT-3 175B의 성능을 크게 상회.
    - **PaLM 540B는 CoT 적용 시 성능이 두 배 이상 향상**.

---

### **2️⃣ Chain-of-Thought Prompting의 주요 실험 결과**

논문의 Figure 4와 Table 2의 실험 결과를 분석하면 **세 가지 핵심 결론**을 도출할 수 있음.

### **🔹 (1) CoT Prompting은 Emergent Ability (창발적 능력)**

- **CoT Prompting의 효과는 모델 크기가 충분히 클 때만 발생**.
    - **100B 이하 모델에서는 효과가 미미함.**
    - 100B 이상의 모델(PaLM 540B 등)에서는 CoT Prompting 적용 시 성능이 급격히 향상됨.
    - 작은 모델에서는 **논리적으로 일관되지 않은 추론을 생성**하여 오히려 성능이 감소하는 경우도 있음.

### **🔹 (2) 문제의 난이도가 높을수록 CoT 효과가 큼**

- GSM8K 같은 **복잡한 문제에서는 CoT 적용 시 성능이 두 배 이상 증가**.
- 반면, SingleOp (단순 연산 문제)에서는 CoT 효과가 거의 없거나, 일부 경우 성능이 감소.
- 즉, **CoT Prompting은 복잡한 다단계 추론 문제에서 가장 효과적**임.

### **🔹 (3) CoT를 적용한 모델은 Fine-tuned 모델과 경쟁 가능**

- **GPT-3 175B, PaLM 540B 등에서 CoT 적용 시 기존 Fine-tuned 모델과 동등한 성능**을 보임.
- Fine-tuning 없이도 강력한 성능을 낼 수 있다는 점에서 **실용적인 이점이 큼**.

---

## **🔹 CoT Prompting의 오류 분석**

논문에서는 **CoT Prompting이 틀린 답을 생성하는 경우**를 분석함.

- **정답을 맞춘 경우 (50개 중 48개 정확한 논리적 CoT 생성)**
    - 대부분의 경우, 모델이 올바른 수학적 논리를 따름.
- **오답을 생성한 경우 (50개 중 46%는 단순 오류, 54%는 심각한 논리적 오류)**
    - 46%는 **소수점 오차, 계산기 오류, 기호 매핑 오류 등 사소한 실수**.
    - 54%는 **문제 이해 자체의 오류 또는 비논리적인 CoT 생성**.
    - 특히, 작은 모델(예: PaLM 62B)에서는 논리적 일관성이 부족한 경우가 많음

---

## **🔹 Ablation Study (소거 실험)**

논문의 3.3절에서는 **Chain-of-Thought (CoT) Prompting이 실제로 효과적인 이유를 확인하기 위해 다양한 변형을 테스트하는 소거 실험(Ablation Study)**을 수행하였습니다.

### **🔹 실험 목표**

- **CoT Prompting의 성능 향상이 자연어 기반 reasoning 때문인지, 아니면 다른 요인 때문인지 확인.**
- **다양한 변형을 실험하여 CoT가 실제로 reasoning 능력을 강화하는지 검증.**

---

### **✅ 1️⃣ Equation-Only Prompting (수식만 생성하는 프롬프트)**

**🔹 실험 개요**

- CoT Prompting이 도움이 되는 이유 중 하나는 **중간 추론 과정에서 수식을 생성하기 때문일 수 있음.**
- 이를 검증하기 위해, **CoT를 제거하고 모델이 직접 수식만 출력한 후 답을 생성하도록 유도.**

**🔹 실험 결과**

- **GSM8K(복잡한 수학 문제)에서는 성능이 거의 향상되지 않음.**
    - 문제의 의미를 해석하지 않고 바로 수식으로 변환하는 것이 어려움.
- **반면, SingleOp(단순 연산 문제)에서는 성능이 향상됨.**
    - 한두 단계의 단순한 문제에서는 수식만으로도 쉽게 해결 가능하기 때문.

**🔹 결론**

- **CoT의 효과는 단순히 수식을 생성하는 것이 아니라, 자연어 기반의 추론 과정 자체에 있음.**
- **복잡한 reasoning이 필요한 문제에서는 자연어 추론 과정(CoT)이 필수적.**

---

### **✅ 2️⃣ Variable Compute-Only Prompting (계산량 증가만으로 성능 향상 여부 확인)**

**🔹 실험 개요**

- CoT Prompting이 성능을 향상시키는 이유 중 하나가 **더 많은 계산량(토큰 수)을 소비하도록 유도하기 때문일 수도 있음.**
- 이를 검증하기 위해, **CoT를 제거하고, 대신 문제를 해결하는 데 필요한 계산량만큼 '...'을 출력하도록 함.**

**🔹 실험 결과**

- **기존 Baseline과 비교했을 때 성능이 거의 동일.**
    - 즉, 단순히 더 많은 토큰을 생성한다고 reasoning 능력이 향상되는 것은 아님.

**🔹 결론**

- **CoT의 성능 향상은 단순한 계산량 증가 때문이 아니라, 실제로 자연어를 사용한 논리적 reasoning이 영향을 줌.**

---

### **✅ 3️⃣ Chain of Thought After Answer (정답 이후에 CoT 추가)**

**🔹 실험 개요**

- CoT Prompting이 성능을 향상시키는 이유가 **단순히 LLM이 사전 훈련된 지식을 더 잘 활용하도록 만드는 것일 수도 있음.**
- 이를 검증하기 위해, **CoT를 정답 이후에 제공하는 변형을 실험.**

**🔹 실험 결과**

- **Baseline과 거의 동일한 성능을 보임.**
    - 즉, **CoT가 reasoning 과정 자체를 보조해야만 성능이 향상됨.**
    - 단순히 정답 이후에 부가적인 설명을 추가하는 것은 의미 없음.

**🔹 결론**

- **CoT Prompting의 효과는 reasoning 과정에서 단계적으로 사고하는 것에 있음.**
- **정답 이후에 reasoning을 추가하는 것은 reasoning 능력을 향상시키지 않음.**

---

## **🔹 Robustness of Chain of Thought (CoT의 강건성 실험)**

이 실험에서는 **CoT Prompting이 얼마나 다양한 환경에서도 일관된 성능을 유지할 수 있는지**를 검증했습니다.

---

### **✅ 1️⃣ 다른 주석자(Annotators)들이 작성한 CoT 비교**

**🔹 실험 개요**

- CoT의 성능이 특정 스타일의 자연어 표현에만 의존하는지 확인하기 위해,
    - **논문의 공동 저자 3명(Annotators A, B, C)이 서로 다른 스타일로 CoT를 작성.**
    - 또한, A는 간결한 CoT 스타일도 추가 작성.

**🔹 실험 결과**

- **어떤 주석자가 작성한 CoT를 사용하더라도 성능이 향상됨.**
- **즉, 특정한 언어적 스타일이 아니라, 논리적 추론 과정 자체가 CoT의 핵심임.**

---

### **✅ 2️⃣ GSM8K 데이터셋에서 랜덤으로 선택한 예제들로 실험**

**🔹 실험 개요**

- 실험에서 사용한 few-shot 예제가 특별히 CoT 효과를 증폭하는 것인지 확인하기 위해,
    - **GSM8K 데이터셋에서 무작위로 8개 예제를 선택하여 실험.**

**🔹 실험 결과**

- **수동으로 작성한 CoT와 거의 동일한 성능을 기록.**
- **즉, 특정 예제에 의존하지 않고, 다양한 예제에서도 CoT의 효과가 유지됨.**

---

### **✅ 3️⃣ Exemplars 순서와 개수 변화 실험**

**🔹 실험 개요**

- **Few-shot 예제의 순서와 개수가 CoT Prompting 성능에 영향을 주는지 분석.**

**🔹 실험 결과**

- **순서를 변경해도 CoT Prompting 성능은 크게 변화하지 않음.**
- **예제 개수를 줄이면 성능이 감소하지만, 기존 Prompting보다 여전히 높은 성능을 유지.**

---

## **🔹 결론 및 시사점**

### **✅ CoT Prompting의 효과를 검증한 핵심 결과**

1️⃣ **CoT의 성능 향상은 자연어 기반 reasoning 덕분이며, 단순히 수식을 생성하거나 계산량을 늘리는 것 때문이 아님.**

2️⃣ **정답 이후에 reasoning을 추가하는 것은 도움이 되지 않으며, reasoning 과정 자체가 모델이 정답을 도출하는 데 필수적.**

3️⃣ **다른 사람이 작성한 CoT 예제나, 다른 스타일의 CoT를 사용해도 성능이 유지됨.**

4️⃣ **랜덤으로 선택한 예제 세트에서도 CoT Prompting이 안정적으로 성능을 향상시킴.**

### **✅ CoT Prompting의 실용적 의미**

- **특정한 언어 스타일에 의존하지 않으므로, 다양한 환경에서 활용 가능.**
- **Prompt Engineering을 통해 최적의 예제를 찾을 필요 없이, 일반적인 CoT를 적용해도 성능 향상이 가능.**
- **Few-shot Prompting 방식에서도 강력한 reasoning 능력을 부여할 수 있음.**

---

## **🔹 Commonsense Reasoning (상식적 추론)**

**Chain-of-Thought (CoT) Prompting은 수학 문제뿐만 아니라, 상식적 추론(commonsense reasoning) 문제에도 적용 가능**하다는 것을 입증하기 위해 실험을 수행하였습니다.

---

### **🔹 상식적 추론 문제란?**

- **사람들이 일반적으로 알고 있는 물리적 법칙과 인간 행동 패턴을 기반으로 논리적 결론을 도출하는 문제.**
- 기존 NLP 모델이 여전히 해결하기 어려운 영역(Talmor et al., 2021).

### **🔹 실험에 사용된 벤치마크 데이터셋**

다양한 유형의 상식적 추론을 다루기 위해 **5개의 데이터셋**을 선정하였습니다.

| 데이터셋 | 설명 |
| --- | --- |
| **CSQA (CommonsenseQA)** | 일반적인 상식 기반 질문에 답하는 데이터셋 (Talmor et al., 2019). |
| **StrategyQA** | 다단계 추론이 필요한 문제 (Geva et al., 2021). |
| **BIG-bench - Date Understanding** | 주어진 문맥에서 날짜를 추론하는 문제. |
| **BIG-bench - Sports Understanding** | 스포츠 관련 문장이 타당한지 판단하는 문제. |
| **SayCan** | 자연어 명령을 로봇 행동 시퀀스로 변환하는 문제 (Ahn et al., 2022). |

➡ **CoT Prompting은 수학 문제뿐만 아니라, 상식적 추론이 필요한 다양한 문제에서도 적용 가능함을 확인.**

---

### **🔹 4.3 실험 방법**

- **CSQA, StrategyQA, SayCan**: 학습 데이터에서 무작위로 샘플을 선택한 후, 직접 Chain-of-Thought (CoT)를 작성하여 few-shot 예제로 사용.
- **BIG-bench Date Understanding, Sports Understanding**: 훈련 데이터가 없으므로 평가 데이터에서 처음 10개 샘플을 선택하여 few-shot 예제로 사용.

---

### **🔹 4.4 실험 결과**

- **모델 크기가 증가할수록 성능 향상**
    - **PaLM 540B 모델이 가장 큰 성능 개선을 보임.**
    - CoT Prompting을 적용하면 일반 Prompting보다 더 높은 성능을 기록.
- **StrategyQA에서 SOTA 달성**
    - **기존 최고 성능 69.4% → CoT 적용 후 75.6%로 향상.**
- **Sports Understanding에서 인간 전문가보다 높은 성능 기록**
    - **CoT 적용 후 95.4% 성능 달성 (인간 전문가 84%보다 높음).**
- **CSQA에서는 성능 향상이 미미함.**
    - 일반적인 상식 질문에서는 CoT Prompting이 큰 이점을 제공하지 못함.
    - **이유:** CSQA는 단순한 상식 질문이 많아, 단계적 reasoning이 필요하지 않기 때문.

➡ **결론:**

- **CoT Prompting은 복잡한 reasoning이 필요한 상식 문제에서 가장 큰 효과를 발휘.**
- **특히, 다단계 reasoning이 필요한 StrategyQA 같은 데이터셋에서 강력한 성능을 보임.**

---

## **🔹 Symbolic Reasoning (기호 논리적 추론)**

**CoT Prompting이 단순한 언어 기반 reasoning뿐만 아니라, 기호 논리(symbolic reasoning) 문제에서도 효과적인지 검증.**

---

### **🔹 기호 논리적 추론 문제란?**

- 기호(Symbol) 조작을 포함하는 문제로, 사람이 보기에 단순하지만 **LLM에게는 어려운 문제**.
- NLP 모델이 보지 못한 새로운 기호 조합을 일반화하는 능력을 평가할 수 있음.

### **🔹 실험에 사용된 과제**

| 과제 | 설명 |
| --- | --- |
| **Last Letter Concatenation** | 주어진 단어들의 마지막 글자를 추출해 연결하는 문제 (예: "Amy Brown" → "yn"). |
| **Coin Flip** | 동전이 여러 번 뒤집어졌을 때, 최종적으로 앞면인지 뒷면인지 예측하는 문제. |

➡ **CoT Prompting을 적용하면 기호적 조작이 필요한 문제에서도 reasoning 성능이 향상되는지 평가.**

---

### **🔹 실험 방법**

- **In-domain 설정**: Few-shot 예제에서 본 것과 동일한 복잡도를 가진 테스트 데이터 사용.
- **Out-of-Domain (OOD) 설정**: Few-shot 예제보다 더 긴 단계의 reasoning이 필요한 데이터를 테스트.
    - 예: **Last Letter Concatenation**에서는 2개의 단어로 된 이름만 학습하고, 3~4개의 단어로 된 이름을 테스트.
    - **Coin Flip**에서는 단순한 플립만 학습하고, 더 많은 플립이 포함된 데이터를 테스트.

---

### **🔹 실험 결과**

### **✅ 1️⃣ In-domain 결과**

- **PaLM 540B + CoT Prompting 적용 시 거의 100% 정답률 달성.**
    - CoT가 있는 경우, 기존 Prompting보다 압도적으로 높은 성능을 기록.
    - 특히, Last Letter Concatenation은 일반 Prompting으로는 거의 해결 불가능하지만, CoT를 적용하면 쉽게 해결됨.

➡ **CoT가 학습된 구조를 잘 따라가며, 기호적 조작 문제도 해결 가능함을 확인.**

### **✅ 2️⃣ Out-of-Domain (OOD) 결과**

- **기존 Prompting 방식은 실패**
    - Few-shot 예제보다 더 긴 문제(예: 3~4단어 Last Letter Concatenation)에서는 일반 Prompting이 완전히 실패.
- **CoT Prompting은 OOD 문제에서도 점진적으로 성능 향상**
    - 학습한 예제보다 더 긴 reasoning이 필요할 때도 일정 수준 이상 성능을 유지.
    - **즉, CoT는 reasoning의 일반화 능력을 향상시킴.**

➡ **결론:**

- **CoT Prompting은 단순한 상식적 reasoning뿐만 아니라, 기호 논리적 reasoning에서도 강력한 성능 향상을 보임.**
- **특히, 새로운 패턴을 일반화해야 하는 Out-of-Domain 상황에서도 효과적.**

---

## **🔹 최종 결론 및 시사점**

### **✅ Chain-of-Thought Prompting의 핵심 정리**

1️⃣ **CoT Prompting은 상식적 reasoning에서도 효과적**

- 특히 **다단계 reasoning이 필요한 문제에서 큰 성능 향상**
- **StrategyQA에서는 기존 최고 성능(SOTA) 달성**
- **Sports Understanding에서는 인간 전문가보다 높은 성능 기록**

2️⃣ **기호 논리적 reasoning에서도 CoT가 효과적**

- 단순한 기호 변환 문제(Last Letter Concatenation, Coin Flip)에서도 일반 Prompting보다 뛰어난 성능.
- 특히, **Out-of-Domain 테스트에서도 reasoning 능력을 일반화할 수 있음.**
