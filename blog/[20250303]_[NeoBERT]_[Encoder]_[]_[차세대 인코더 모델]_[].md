[https://arxiv.org/html/2502.19587v1#S4](https://arxiv.org/pdf/2502.19587v1)

## **NeoBERT의 핵심 요약**

NeoBERT는 기존 BERT/RoBERTa 기반 인코더 모델을 최신 기술과 데이터로 업데이트한 **차세대 인코더 모델**입니다.

✅ **더 긴 컨텍스트 길이 (4096 토큰)**

✅ **최신 Transformer 아키텍처 적용 (RoPE, SwiGLU, RMSNorm)**

✅ **대규모 최신 데이터 (RefinedWeb, 600B 토큰) 사용**

✅ **최적의 깊이-너비 비율 적용 (28 layers, 768 hidden size)**

✅ **최신 최적화 기법 (AdamW, FlashAttention, DeepSpeed) 적용**

✅ **오픈소스 모델로 공개 (코드, 데이터, 체크포인트 포함)**

NeoBERT는 기존 모델 대비 **더 작은 크기(250M 파라미터)로도 강력한 성능을 제공**하며, 특히 **GLUE 및 MTEB 벤치마크에서 기존 모델들을 뛰어넘는 성과**를 기록했습니다.

---

## **1. NeoBERT란?**

NeoBERT는 기존의 BERT 계열 모델(BERT, RoBERTa 등)을 최신 트렌드에 맞춰 발전시킨 차세대 인코더 모델입니다.

- **배경:** LLaMA, DeepSeek 같은 **디코더 기반 LLM**이 크게 발전했지만, **BERT 같은 인코더 기반 모델은 발전이 미미**했습니다.
- **목적:** NeoBERT는 **최신 아키텍처 개선, 현대적 데이터셋, 최적화된 사전 학습 기법**을 적용해 인코더 모델을 강화하는 것을 목표로 합니다.
- **장점:** 기존의 BERT를 대체할 수 있도록 **동일한 구조**를 유지하면서도 더 **긴 컨텍스트(4096 토큰)**와 **최신 학습 기법**을 활용합니다.
- **성능:** 250M 파라미터로도 기존 BERTlarge, RoBERTalarge, NomicBERT, ModernBERT보다 뛰어난 성능을 보이며 **MTEB 벤치마크에서 최고 성능**을 기록했습니다.

---

## **2. 기존 BERT의 한계**

- **사전학습 후 미세조정(Fine-tuning)의 중요성**
    - BERT 계열 모델은 **마스킹된 단어 복원(Masked Language Modeling, MLM)**을 통해 사전학습됨.
    - 그러나 **이 방식만으로는 실제 응용에 적합한 표현 학습이 부족**하여 추가적인 대조 학습(contrastive learning)이 필요함.
    - GTE, jina-embeddings, SFR-embeddings 등 최근의 모델들은 주로 **미세조정 기법을 개선하는 데 집중했을 뿐, 인코더 자체의 구조를 개선하지 않음**.
- **기존 모델이 사용하는 데이터가 오래됨**
    - BERT는 **2019년 데이터**(BookCorpus, Wikipedia) 기반으로 학습됨 → 최신 언어 트렌드 반영 부족
    - RoBERTa는 데이터 크기를 키웠지만, **현대 웹 데이터를 반영하지 못함**.
    - 최신 NomicBERT는 데이터를 2023년 기준으로 확장했지만, 아직 최적화 부족.
- **길이 제한(Sequence Length) 문제**
    - 기존 BERT/RoBERTa는 **최대 512 토큰**까지만 처리 가능 → 장문 문서 처리 한계
    - NomicBERT는 이를 **2048 토큰**까지 확장했지만, 여전히 제한적
- **비효율적인 아키텍처**
    - **너비(Width)에 비해 깊이(Depth)가 비효율적**으로 설계됨 → 최적의 깊이-너비 비율을 찾아야 함
    - 기존 모델들은 RoPE(회전 위치 인코딩), RMSNorm(루트 평균 제곱 정규화) 같은 최신 기술을 적용하지 않음

---

## **3. NeoBERT의 주요 개선점**

NeoBERT는 최신 연구를 반영하여 **더 효율적인 구조, 더 좋은 데이터, 개선된 학습 기법**을 적용했습니다.

### **① 아키텍처 개선**

| 모델 | 레이어 수 | 히든 크기 | 컨텍스트 길이 |
| --- | --- | --- | --- |
| BERT | 12 | 768 | 512 |
| RoBERTa | 12 | 768 | 512 |
| NomicBERT | 12 | 768 | 2048 |
| NeoBERT | 28 | 768 | 4096 |
- **깊이(Depth) 최적화:**
    - 기존 BERT는 **너비(Width)가 비효율적**으로 설계됨
    - NeoBERT는 **최적의 깊이-너비 비율을 찾아 28개 레이어를 적용**
- **위치 인코딩(Positional Encoding) 개선:**
    - 기존 BERT는 **절대적 위치 임베딩(Absolute Positional Embedding)**을 사용 → 긴 문장에 대한 일반화가 어려움
    - NeoBERT는 **RoPE(Rotary Position Embeddings)** 적용 → 긴 문장에서도 성능 유지
- **활성화 함수 변경:**
    - 기존 BERT: **GeLU**
    - NeoBERT: **SwiGLU** (LLM에서 성능 향상 검증됨)
- **정규화 방식 개선:**
    - 기존 BERT: **Post-LayerNorm**
    - NeoBERT: **RMSNorm** → 더 빠르고 안정적인 학습 가능

---

### **② 데이터 및 학습 방식 개선**

- **더 많은 데이터 사용:**
    - 기존 BERT는 13GB 데이터셋 사용, RoBERTa는 160GB 사용
    - NeoBERT는 최신 웹 데이터 RefinedWeb(600B 토큰) 사용 → 최신 언어 패턴 반영
- **긴 컨텍스트 학습:**
    - 1단계: **1024 토큰**으로 1M 스텝 학습
    - 2단계: **4096 토큰**으로 추가 학습 → 긴 문장에서도 성능 유지

---

### **③ 사전 학습(Pre-training) 최적화**

- **마스킹 비율 조정:**
    - 기존 BERT는 **15% 마스킹**
    - NeoBERT는 **20% 마스킹**으로 조정 → 더 어려운 학습 과제 부여하여 모델 성능 향상
- **더 많은 토큰 학습:**
    - 기존 BERT: 131B 토큰 학습
    - NeoBERT: 2.1T 토큰 학습 → 성능 최적화

---

### **④ 효율적인 연산 및 최적화**

- **FlashAttention 적용:**
    - 기존 BERT의 **O(N²) 메모리 사용 문제 해결**
    - FlashAttention을 적용해 **메모리 사용량 감소 + 연산 속도 향상**
- **DeepSpeed 및 ZeRO 최적화 사용:**
    - **GPU 병렬화 최적화** → 더 큰 배치 크기(2M 토큰)로 학습 가능

---

## **4. 주요 설계 선택과 효과 분석 (Ablation Study)**

NeoBERT의 개선 사항이 실제 성능에 미친 영향을 분석하기 위해 **총 10가지 모델 변형(M0~M10)**을 학습하여 실험했습니다.

모든 모델은 **동일한 학습 환경**에서 훈련되었으며, 각 변경 사항이 GLUE 벤치마크 성능에 미치는 영향을 평가했습니다.

### **🔹 성능 향상에 가장 큰 영향을 준 요소**

| 실험 | 변경 사항 | GLUE 성능 향상 |
| --- | --- | --- |
| M2 | **더 큰 데이터셋 (RefinedWeb, 600B 토큰) 적용** | **+3.6%** |
| M7 | **모델 크기 증가 (120M → 250M)** | **+2.9%** |
1. **데이터 크기 증가 (M2, +3.6%)**
    - 기존 BERT가 사용한 Wikipedia, BookCorpus 대신 **더 크고 다양한 데이터셋(RefinedWeb)**을 활용
    - 최신 언어 패턴을 더 잘 반영하면서도 성능 향상
2. **모델 크기 증가 (M7, +2.9%)**
    - 120M 파라미터에서 250M 파라미터로 증가하면서 성능 개선
    - 단순히 크기만 키운 것이 아니라, **깊이-너비 비율을 최적화**

---

### **🔹 성능 하락을 유발한 요소**

| 실험 | 변경 사항 | GLUE 성능 하락 |
| --- | --- | --- |
| M3 | **LLaMA 2의 BPE 토크나이저 사용** | **-2.1%** |
| M6 | **시퀀스 패킹 적용 (un-padding)** | **-2.9%** |
1. **LLaMA 2의 BPE 토크나이저 적용 (M3, -2.1%)**
    - 기존 Google WordPiece 토크나이저 대신 LLaMA 2의 BPE 적용
    - 결과적으로 인코더 모델에서는 **더 압축된 표현을 만드는 데 불리**하여 성능 저하
2. **시퀀스 패킹 적용 (M6, -2.9%)**
    - 패딩된 토큰을 제거하여 메모리 낭비를 줄이려 했으나, **크로스-시퀀스 간 주의(attention) 정보 손실**로 성능 감소
    - 하지만, 이 개념은 향후 정확한 크로스-어텐션을 고려한 형태로 개선될 가능성 존재

---

### **🔹 성능 하락에도 불구하고 유지한 요소**

| 실험 | 변경 사항 | GLUE 성능 변화 |
| --- | --- | --- |
| M4 | **AdamW 옵티마이저 + 코사인 디케이 적용** | **-0.5%** |
| M5 | **마스킹 비율 증가 (15% → 20%)** | **-0.7%** |
1. **AdamW + 코사인 학습률 스케줄링 (M4, -0.5%)**
    - 초기에는 성능이 감소했지만, 장기적인 관점에서 **과적합 방지 및 일반화 성능 향상** 기대
2. **마스킹 비율 증가 (M5, -0.7%)**
    - 마스킹 비율을 15%에서 20%로 증가 → 모델이 더 어려운 학습을 하게 됨
    - 초기에는 성능이 감소했지만, **대규모 학습 시 유리할 것으로 예상**

---

## **5. 실험 결과 (GLUE 및 MTEB 평가)**

### **🔹 GLUE 벤치마크 성능 비교**

NeoBERT는 기존 모델 대비 **훨씬 작은 파라미터 수(250M)**로도 뛰어난 성능을 보였습니다.

| 모델 | GLUE 점수 | 파라미터 수 |
| --- | --- | --- |
| BERTbase | **79.6%** | **120M** |
| RoBERTabase | **86.4%** | **125M** |
| NomicBERT2048 | **84.0%** | **137M** |
| ModernBERT | **88.5%** | **149M** |
| **NeoBERT1024** | **88.8%** | **250M** |
| **NeoBERT4096** | **89.0%** | **250M** |

✅ **NeoBERT는 ModernBERT와 비슷한 성능을 유지하면서도 100M 작은 파라미터로 구현됨**

✅ **RoBERTaLarge(350M 파라미터)보다 작은 크기로도 더 높은 성능을 기록**

---

### **🔹 MTEB 벤치마크 성능 비교**

MTEB(Massive Text Embedding Benchmark)은 더 다양한 NLP 태스크(클러스터링, 검색, STS 등)를 포함한 벤치마크입니다.

| 모델 | MTEB 점수 |
| --- | --- |
| BERT | **48.1%** |
| RoBERTa | **47.7%** |
| NomicBERT | **47.1%** |
| ModernBERT | **45.0%** |
| **NeoBERT4096** | **51.3%** |

✅ **NeoBERT가 모든 기존 모델을 뛰어넘으며 최고 성능 기록 (+4.5% 향상)**

✅ **특히 검색, STS 등에서 더 강력한 성능을 보임**

---

## **6. 효율성 및 속도 비교**

NeoBERT는 4096 토큰까지 처리할 수 있는 **긴 컨텍스트를 지원하면서도 더 빠른 속도**를 기록했습니다.

| 모델 | 처리 속도 (tokens/sec) |
| --- | --- |
| BERTbase | **빠름 (512 토큰까지 최적화됨)** |
| RoBERTabase | **빠름** |
| NomicBERT | **느림 (2048 토큰 지원)** |
| **NeoBERT4096** | **최고 속도 (4096 토큰 지원)** |

✅ **4096 토큰 이상에서는 NeoBERT가 가장 빠름 (ModernBERT 대비 46.7% 속도 향상)**

✅ **FlashAttention, DeepSpeed 병렬화 적용 덕분에 대용량 데이터도 빠르게 처리 가능**

---

## **7. 결론**

NeoBERT는 **기존 인코더 모델들의 한계를 극복**하며, **더 작은 모델 크기로 더 강력한 성능을 제공**하는 차세대 BERT 모델입니다.

📌 **더 긴 컨텍스트 지원 (4096 토큰)**

📌 **최신 Transformer 기술 적용 (RoPE, SwiGLU, RMSNorm 등)**

📌 **대규모 최신 데이터 활용 (RefinedWeb, 600B 토큰)**

📌 **GLUE 및 MTEB 벤치마크에서 기존 모델 대비 뛰어난 성능**

📌 **최고 수준의 효율성과 속도를 제공 (FlashAttention, DeepSpeed 최적화)**

📌 **완전 오픈소스로 제공 (재현 가능성 보장)**

👉 **NeoBERT는 차세대 NLP 태스크에서 강력한 인코더 모델로 자리 잡을 가능성이 큽니다.** 🚀
