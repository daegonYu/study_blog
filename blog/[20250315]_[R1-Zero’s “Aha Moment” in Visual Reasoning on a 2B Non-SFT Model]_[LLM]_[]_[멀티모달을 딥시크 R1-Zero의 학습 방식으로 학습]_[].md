# R1-Zero’s “Aha Moment” in Visual Reasoning on a 2B Non-SFT Model

https://arxiv.org/html/2503.05132v2

![image](https://github.com/user-attachments/assets/fa04dcf6-bb79-490e-b2a8-7d24bfbf166f)


## 📌 논문의 주요 내용 요약

이 논문은 **DeepSeek-R1** 모델이 보였던 "Aha Moment"라는 개념을 **멀티모달(텍스트+이미지) 모델**에서도 재현하려는 시도입니다. 연구팀은 **Qwen2-VL-2B**라는 **2B(20억 개 파라미터) 모델**에 대해 **강화 학습(Reinforcement Learning, RL)** 을 적용하여 이런 특성을 성공적으로 재현했다고 주장합니다.

### 🎯 주요 연구 목표

1. **멀티모달 환경에서도 "Aha Moment"를 재현**
    - "Aha Moment"란 모델이 **스스로 학습하면서 갑자기 문제 해결 능력이 향상되는 순간**을 의미합니다.
    - 이는 모델이 자율적으로 더 깊은 사고를 하게 되는 현상입니다.
2. **Supervised Fine-Tuning(SFT) 없이도 강력한 추론 능력 확보**
    - 기존 연구들은 주로 **지도 학습(SFT)** 을 활용해 모델을 강화했으나, 본 연구에서는 이를 사용하지 않고 RL만으로 성능을 높였습니다.
    - 이를 통해 기존 대비 **30% 향상된 성능**을 달성하였고, SFT 모델 대비 **2% 더 높은 정확도**를 기록했습니다.
3. **강화 학습이 Instruct 모델에서는 효과적이지 않음을 발견**
    - **Supervised Fine-Tuned(SFT) 모델에 RL을 적용하면, 모델이 단순한 패턴을 반복할 뿐, 진정한 추론 능력 향상은 이루어지지 않음**을 발견했습니다.
    - 또한 **"응답 길이 증가"라는 단순한 보상 방식은 깊은 추론 능력을 유도하는 데 효과적이지 않음**을 확인했습니다.

---

## 🔬 논문의 핵심 개념

### 🔹 **DeepSeek-R1의 "Aha Moment"란?**

DeepSeek-R1 모델은 RL을 통해 자율적으로 학습하면서 **갑자기 스스로 더 깊은 사고를 하는 순간을 맞이**하게 됩니다.

이 현상을 "Aha Moment"라고 부릅니다.

즉, 학습 도중 특정 시점에서 모델이 **문제 해결 방식을 스스로 개선**하는 순간이 온다는 것이죠.

> ✔ 일반적인 모델: 데이터에 맞춰 주어진 답을 예측
> 
> 
> ✔ **DeepSeek-R1 모델**: 스스로 "생각"하고, 점점 더 복잡한 문제를 해결
> 

---

### 🔹 **멀티모달 환경에서 "Aha Moment"를 재현한 방법**

기존 연구들은 DeepSeek-R1의 성공을 **텍스트 기반 모델**에서만 확인했으나,

본 연구에서는 **멀티모달 모델(Qwen2-VL-2B)** 에 RL을 적용해 비슷한 현상을 유도하려 했습니다.

1. **강화 학습을 SAT 데이터셋에 직접 적용**
    - SAT 문제(텍스트+이미지 포함)를 사용하여 RL을 수행
    - 초기에는 **HTML 코드 같은 불필요한 출력을 하던 모델이 RL을 통해 자연스러운 응답 방식으로 변화**함
2. **"Aha Moment"와 응답 길이 증가 확인**
    - 훈련 도중 특정 시점부터 모델이 스스로 더 깊이 사고하는 모습을 보임
    - 이와 함께 응답 길이가 점진적으로 증가

---

### 🔹 **RL이 Instruct 모델에서 실패한 이유**

기존 SFT 모델에 RL을 적용하면, **단순히 길이를 늘리는 패턴을 학습**할 뿐 진짜 추론 능력은 향상되지 않는 문제가 있었습니다.

✔ **발견된 문제점**

1. RL이 **Instruct 모델에서는 표면적인 논리만 강화**하고, 진짜 문제 해결 능력을 학습하지 못함

![image](https://github.com/user-attachments/assets/409399f4-b69f-4b1a-be1e-d7923b42c557)


**Instruct 모델에 3가지 모델에 파인튜닝 진행 결과 학습이 진행될수록 답변 길이가 짧아지는 현상 발생 → 따라서 길이를 보상으로 주는 보상 함수 추가**

1. **길이를 보상으로 주는 보상 함수 추가 결과 :** 단순히 **"응답 길이가 길어지는 것"이 깊은 사고를 의미하지 않음**
→ 모델이 그냥 "길게 답변해야 한다"는 규칙만 배우는 문제가 발생

![image](https://github.com/user-attachments/assets/45b4b58e-46a3-424a-8365-1da52bc2e9b9)


✔ **해결책**

- 처음부터 SFT 없이 RL을 적용하면 모델이 **진짜 문제 해결 능력을 학습**할 가능성이 더 높아짐

---

## 🏆 논문의 주요 기여

1️⃣ **멀티모달 모델에서도 "Aha Moment"를 재현**

- 기존 텍스트 기반 연구에서만 발견된 현상을 **비전+텍스트 모델에서도 확인**

2️⃣ **SFT 없이 RL만으로 모델의 추론 능력 향상 가능성 제시**

- 2B 모델에서도 충분한 성능 향상을 달성

3️⃣ **RL이 SFT 모델에서는 효과적이지 않다는 문제점 발견**

- 단순히 Instruct 모델에서 RL을 적용하는 것만으로는 R1과 같은 효과를 얻기 어려움

4️⃣ **오픈소스 프로젝트 제공**

- 연구 결과를 GitHub에 공개하여 후속 연구를 지원

---

## ✨ 결론 및 시사점

✅ **RL만으로도 모델이 스스로 추론 능력을 개선할 수 있다!**

✅ **단순한 길이 보상은 효과적이지 않다. 모델이 진짜로 사고하게 만들어야 한다.**

✅ **멀티모달 환경에서도 "Aha Moment"를 유도할 수 있다.**

이 연구는 **강화 학습을 통한 멀티모달 모델의 자율적 추론 능력 향상**이라는 중요한 시사점을 제공합니다.

추후 연구에서는 **더 큰 모델에서의 적용 가능성, 보상 함수 설계 최적화, 더 나은 RL 방법론 탐색**이 주요 과제가 될 것입니다.
