# LADDER: Self-Improving LLMs Through Recursive Problem Decomposition

https://arxiv.org/html/2503.00735v3

**풀 수 없는 문제를 '체계적으로' 풀 수 있는 하위 문제로 쪼개면서**, 사람의 도움 없이 **자기학습**을 진행할 수 있도록 하는 방법론

---

# 개요

---

## 📌 논문의 핵심 아이디어

이 논문의 핵심은 LLM(대규모 언어 모델)이 **스스로 어려운 문제를 더 쉬운 하위 문제로 쪼개면서 단계적으로 학습하는 방식을 제안**한 것입니다.

지금까지의 강화 학습(RL) 방식은 사람이 제공한 학습 데이터나 난이도가 적절히 설정된 문제들로부터 학습을 했습니다. 하지만 이런 방식은 다음과 같은 한계가 있었습니다.

- 사람이 문제 난이도를 일일이 설정해야 함
- 지나치게 어려운 문제를 풀도록 하면 학습이 무너짐 (catastrophic collapse)
- 난이도 점진성이 확보되지 않으면 학습이 효과적으로 이루어지지 않음

이에 저자들은 **어려운 문제를 풀기 전에 모델이 스스로 간단한 하위 문제를 만들어내고, 그 하위 문제를 풀면서 단계적으로 학습하는 방식**을 제안했습니다. 

---

## 📌 LADDER의 원리 (이름 풀이)

LADDER (**Learning through Autonomous Difficulty-Driven Example Recursion**):

즉, "자율적 난이도 기반 예제 재귀를 통한 학습"입니다. 쉽게 말해, 다음과 같은 순서입니다.

1. **어려운 문제를 발견하면**, LLM에게 이 문제를 직접 풀라고 하지 않고,
2. 모델 스스로에게 **"이 문제의 좀 더 쉬운 버전을 만들어봐"** 라고 요청합니다.
3. 생성된 쉬운 문제에 대해서도 풀 수 없다면, 더 쉬운 하위 문제를 또 생성하도록 합니다.
4. 이런 과정을 재귀적으로 반복하다 보면, **모델이 쉽게 풀 수 있는 난이도의 문제가 생겨납니다.**
5. 이 쉬운 문제를 해결하고, 해결된 결과를 다시 상위 문제 해결의 발판으로 삼아 조금씩 어려운 문제들을 해결해 나갑니다.

이 방식은 결국 **난이도의 자연스러운 계단**(difficulty gradient)을 만들어, 모델이 스스로 성장하도록 유도합니다.

---

## 📌 기존 방식과 차별점

| 기존 방식 | LADDER 방식 |
| --- | --- |
| 사람이 난이도를 정하고 데이터셋을 만들어야 함 | 모델 스스로 난이도별 문제 생성 |
| 사람이 중간중간에 개입해야 함 (피드백 필요) | 모델 스스로 학습 과정을 자율적으로 진행 |
| 데이터셋 구축에 비용이 큼 | 별도의 데이터 구축 필요 없이 자체적으로 문제 생성 |

즉, 사람의 개입 없이, 모델이 스스로 성장의 방향을 결정하고 학습의 계단을 올라가도록 합니다.

---

## 📌 실험 및 결과 요약

저자들은 **수학적 적분 문제(Integration)**를 대상으로 LADDER 방식을 테스트했습니다.

- 모델:
    - Llama 3.2 3B
    - Qwen2.5 7B Deepseek-R1 Distilled (7B)
- 결과:

| 모델과 테스트 | 기존 성능 | LADDER 적용 후 성능 |
| --- | --- | --- |
| Llama 3.2 3B (대학생 수준 적분 문제) | 1% | **82%** |
| Qwen2.5 7B (MIT 적분 경시대회 수준) | 낮음 (약 2% 미만 추정) | **73%** |

→ **대규모 모델 (GPT-4o: 42%) 및 사람(15~30%)보다 훨씬 높은 성능을 기록**

---

## 📌 TTRL (Test-Time Reinforcement Learning)의 개념과 의의

추가로 저자들은 LADDER 방식을 확장하여 **테스트 시점에서도 강화학습을 진행하는 TTRL을 제안**했습니다.

- 기존 강화 학습: 학습 단계에서만 강화학습이 이루어짐.
- TTRL: **추론(inference)** 시점에서 개별 문제마다 문제의 쉬운 변형을 즉시 생성하고 해결하면서 문제를 점진적으로 풀어갑니다.
    
    즉, 추론 과정에서 즉시 문제를 쪼개고, 이를 활용한 강화학습을 수행합니다.
    

이 방식으로 MIT 적분 경시대회에서 정확도를 **73%에서 90%로 끌어올렸으며**, 이는 당시 최고 성능 모델(OpenAI o1)을 뛰어넘는 성과였습니다.

---

# 세부내용

---

## 📌 LADDER 개요

LADDER의 목적은 복잡한 문제를 단계적으로 하위 문제로 쪼개면서 (**재귀적 문제 분해**), 모델이 스스로 학습하도록 돕는 것입니다.

알고리즘 순서는 간단히 다음과 같습니다.

1. 훈련 세트의 문제(적분 문제)가 주어집니다.
2. 각 문제를 점진적으로 간단한 하위 문제들로 변형하여 문제의 **변형 트리(variant tree)**를 만듭니다.
3. 생성된 트리 문제들을 가지고 **강화학습(RL)**을 수행하여 모델의 성능을 높입니다.

이렇게 학습된 모델은 새로운 문제를 더욱 잘 해결하게 됩니다.

---

## 🚩 Algorithm Design (LADDER & TTRL)

논문의 핵심 알고리즘인 **LADDER와 TTRL**을 구성하는 세부 단계를 설명하는 파트입니다.

두 알고리즘은 다음 세 가지 주요 단계로 구성됩니다:

- **(1) 문제 변형 생성 (Variant Generation)**
- **(2) 문제 해결 결과 검증 (Solution Verification)**
- **(3) 강화 학습을 통한 학습 과정 (Reinforcement Learning)**

---

## 📌 (1) Variant Generation (문제 변형 생성)

**변형 생성이란?**

복잡한 문제를 쉽고 간단한 하위 문제로 변형하여, 학습을 쉽게 해주는 과정입니다.

초기엔 단순히 "쉬운 문제를 생성하라"고 지시하는 방식으로 실험했지만, 단조롭고 다양성 부족 문제가 발생했습니다.

**논문에서 제안하는 더 나은 변형 생성법 (3단계 과정):**

### 🔸 **① 수학적 변형 라이브러리 구축**

모델에게 문제를 어떻게 변형할지 수학적으로 다양한 방법을 미리 준비했습니다.

- 간단한 변형: 지수(exponent)를 낮추거나 분모를 단순화 등
- 복잡한 변형: 중첩 함수 추가, 함수 조합 등

이 변형 라이브러리를 기반으로 문제의 난이도를 조절하며 변형합니다.

### 🔸 **② 변형 문제 생성 (배치 단위)**

매 문제마다 랜덤으로 3~5개의 변형 방법을 제안하여 변형을 생성했습니다.

한 번 생성할 때 **10개의 문제를 배치(batch)**로 생성하는 것이 가장 효과적이었습니다.

- 배치 크기가 작으면 반복적 결과가 나왔고,
- 크면 품질이 떨어졌습니다.

또한 변형의 다양성을 높이기 위해 **두 가지 특별한 기법**을 적용했습니다.

| 다양성을 높이는 기법 | 설명 |
| --- | --- |
| **Temperature Cycling** (온도 순환) | 문제 생성 시 창의성과 수학적 정확성을 균형 잡기 위해 샘플링 온도를 0.8~1.4 사이에서 주기적으로 바꿈 |
| **Persona-based Prompting** (다양한 관점의 인물 설정 프롬프팅) | 모델에게 “오일러처럼 급수(series)에 집중하라” 또는 “가우스처럼 패턴을 찾아라” 같은 다양한 수학적 관점을 부여 |

이 기법으로 생성된 문제들이 훨씬 더 다양하고 품질이 좋아졌습니다.

### 🔸 **③ 재귀적 생성으로 난이도 조절**

문제 변형을 1회만 하지 않고, **여러 번 재귀적으로 수행**하여 문제를 **트리(tree)** 구조로 만듭니다.

- 원래 문제를 → 좀 더 간단한 문제로
- 간단한 문제를 → 더 간단한 문제로
- 이를 반복해 최대 3단계로 제한했습니다.

이를 통해 자연스러운 난이도 계단을 만듭니다.

**✅ 품질 관리 문제와 해결 방법**

하지만 모델이 생성한 문제가 오히려 원래보다 어려워질 때가 있었습니다.

이 문제들은 학습 시 **보상을 받지 않아 자연스럽게 배제**됐지만, 계산 자원 낭비라는 문제가 있었습니다.

향후 연구에서 이 문제를 해결하는 방법을 제안할 필요성이 있습니다.

---

## 📌 (2) Solution Verification (해결 결과 검증)

생성된 문제의 해결 결과를 **정확히 검증**하는 과정입니다. 논문에서는 다음과 같은 방식을 사용했습니다.

| 방법 | 설명 |
| --- | --- |
| **다중 샘플링** | [-10, 10] 범위에서 랜덤으로 5개의 지점을 뽑아 원래 문제와 후보해를 비교 |
| **특이점 처리** | 갑작스러운 값 변화가 발생하면 다시 샘플링하여 안정성 유지 |
| **수치적 정확도 기준** | 원래 문제와 후보해를 계산해서 상대 오차가 특정 이하일 때 정답으로 인정 |
| **Timeout 관리** | 최대 2초까지만 계산하고, 초과하면 최대 3번 재시도 후 결과를 판단 |
| **예외적 결과 방지** | 모델이 꼼수를 사용한 답(예: 문제 그대로 출력)을 걸러내는 필터링 추가 |

이러한 엄격한 기준을 통해 매우 높은 품질로 검증을 유지했습니다.

- **수치적(numerical) 검증 방식을 선택한 이유**:
    
    적분 외 다른 문제로 쉽게 확장 가능하기 때문입니다.
    
- 최종 결과(MIT 시험)는 공식 솔루션과 직접 비교하여 평가했습니다.

---

## 📌 (3) RL Protocol (강화학습 프로토콜)

- 변형 문제를 풀면서, 올바른 결과에는 보상을 주고, 틀린 결과는 무시(보상 없음)합니다.
- 이렇게 **보상을 통해 모델이 올바른 풀이 방법을 자연스럽게 배우도록 강화학습**이 이루어집니다.

---

## 📌  강화학습 프로토콜 (Reinforcement Learning Protocol)

이 논문에서는 **GRPO (Group Relative Policy Optimization)** 라는 강화학습 기법을 사용합니다.

**GRPO가 무엇인가?**

- 일반적인 강화학습 알고리즘과 달리 **별도의 critic 모델(기준점 모델)을 쓰지 않고**, 각 질문(question)마다 여러 개의 응답(output)을 생성해 그룹화하고, 이 그룹의 결과를 이용해 보상의 기준점을 정합니다.
- 장점: 계산 효율과 메모리 사용량을 개선합니다.

---

### 🔸 GRPO 알고리즘의 핵심 수식 이해

모델의 학습 목표 (목적함수)는 다음과 같습니다:

![image](https://github.com/user-attachments/assets/0708270a-8efe-41b5-9243-3e6045b2db75)

**GRPO를 사용한 이유?**

- 별도의 Critic 모델 없이 효율적으로 RL 수행 가능.
- 계산 및 메모리 효율성을 높임.

---

### 🔸 Reward 설계 방식 (보상 모델)

단순하고 명료한 규칙 기반의 보상을 사용했습니다.

보상 모델은 다음 두 가지로 구성됩니다:

| 보상 종류 | 설명 |
| --- | --- |
| **Accuracy reward** (정확도 보상) | 생성된 응답이 맞는지 틀린지를 **3.1.3의 Solution Verification 방식**으로 평가 (정답이면 보상) |
| **Format reward** (포맷 보상) | 정답이 반드시 `<ANSWER></ANSWER>` 태그 사이에 정확히 들어있도록 모델을 유도 |
- KL divergence 계수는 0.001로 설정했습니다.
- 학습 배치 크기(batch size)와 epoch 수는 실험마다 다르게 설정합니다.

---

## 📌 Test-Time Reinforcement Learning (TTRL)

TTRL은 LADDER의 아이디어를 추론(테스트) 시점에 적용하여 **모델이 실시간으로 학습하면서 문제를 풀게 하는 기법**입니다.

**TTRL이 무엇인가?**

어려운 문제를 만났을 때, 모델이 바로 그 시점에:

1. 문제를 여러 쉬운 문제들로 분해 (variant tree 생성)
2. 즉석에서 해당 문제들로 RL 학습 진행
3. **문제를 즉시 잘 풀 수 있도록 모델 성능을 동적으로 향상시킵니다.**

즉, **추론할 때마다 모델이 문제 맞춤형 전문가**가 되도록 하는 것입니다.

### 🔸 TTRL의 절차

테스트 문제마다 다음의 과정을 수행합니다.

| 단계 | 내용 |
| --- | --- |
| ① 문제 변형 트리 생성 | 주어진 어려운 테스트 문제를 여러 단계로 쪼개어 쉬운 문제 생성 |
| ② 실시간 RL 학습 | 바로 이 변형 트리 문제들로 RL 프로토콜 수행하여 모델의 능력을 즉석에서 높임 |
| ③ 문제 풀이 후 초기화 | 문제 해결 후, 모델 파라미터를 원래 상태로 되돌리고 다음 문제로 진행 |
- **TTRL의 장점**: 별도의 추가 데이터셋이나 아키텍처 변경 없이, 추론 시점에 컴퓨팅 자원을 효율적으로 써서 모델의 성능을 즉각적으로 높일 수 있습니다.

---

## 📌 LADDER와 TTRL 비교 요약

| LADDER | TTRL |
| --- | --- |
| **학습 시점(Training-time)** 적용 | **테스트 시점(Test-time)** 적용 |
| 전체 데이터셋의 문제를 미리 변형하여 RL 수행 | 매 테스트 문제마다 실시간 변형 생성 및 즉석 RL 수행 |
| 일반화된 능력을 키우는 게 목표 | 특정 문제를 즉시 해결할 능력을 빠르게 강화하는 게 목표 |
