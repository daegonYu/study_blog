https://arxiv.org/html/2412.04862v1#abstract

## 🔹 **EXAONE 3.5: 현실 세계에서 활용 가능한 LLM**

LG AI Research가 개발한 EXAONE 3.5는 다양한 크기의 인스트럭션 튜닝된 LLM(대규모 언어 모델) 시리즈입니다.

### **📌 모델 특징**

- **세 가지 크기 제공**:
    - **32B (최고 성능)**
    - **7.8B (균형 잡힌 성능)**
    - **2.4B (저사양 디바이스 최적화)**
- **긴 문맥 이해**: 최대 **32K 토큰**까지 처리 가능
- **실제 환경에서 뛰어난 성능**: 7개 벤치마크에서 최고 점수 기록
- **SOTA 모델들과 경쟁 가능**: 9개 일반 벤치마크에서 동급 크기의 최신 모델과 비교해 우수한 성능

---

## 🔹 **모델 구조 및 학습 방법**

### **1️⃣ 모델 구조**

- **디코더-온리 트랜스포머 구조**
- **SwiGLU 활성화 함수 사용**
- **GQA (Grouped Query Attention) 적용**: 효율적인 연산 수행
- **RoPE (Rotary Positional Embedding) θ 값 1,000,000 사용**: 긴 문맥 처리 능력 향상

| 모델 크기 | 파라미터 수 | 레이어 수 | 컨텍스트 길이 |
| --- | --- | --- | --- |
| 32B | 32B | 64 | 32K |
| 7.8B | 7.8B | 32 | 32K |
| 2.4B | 2.4B | 30 | 32K |

---

### **2️⃣ 사전 학습 (Pre-training)**

- **1단계 학습**: 광범위한 웹 데이터를 활용한 일반적인 성능 향상
- **2단계 학습**: 특정 도메인 및 긴 문맥 이해 능력 향상

| 모델 크기 | 학습 토큰 수 | 연산량 (FLOPs) |
| --- | --- | --- |
| 32B | 6.5T | 1.25 × 10¹²⁴ |
| 7.8B | 9T | 4.21 × 10¹²³ |
| 2.4B | 6.5T | 9.36 × 10¹²² |

> 📝 Qwen 2.5 32B 모델과 비교했을 때
> 
> - EXAONE 3.5 32B는 **2.77배 적은 연산량**으로 유사한 성능 달성
> - 비용 대비 높은 효율성

### **3️⃣ 컨텍스트 길이 확장**

- 긴 문맥을 잊어버리는 **"망각 문제(catastrophic forgetting)"** 해결→ 1단계 데이터 일부를 **재사용하는 Replay-based 학습 기법** 적용
- 첫 번째 학습에서는 문서를 짧게 쪼개지만, 두 번째 학습에서는 **원본 문서를 유지하여 긴 문맥 처리 능력** 강화

### **4️⃣ 데이터 정제 (Decontamination)**

- 웹 크롤링 데이터에는 벤치마크 테스트셋과 중복되는 데이터가 포함될 가능성이 큼
- **서브스트링 매칭 기법**을 활용하여 훈련 데이터에서 중복 데이터를 철저히 제거

---

## 🔹 **사후 학습 (Post-training)**

### **1️⃣ 지도 미세 조정 (Supervised Fine-tuning, SFT)**

- 다양한 분야의 데이터(8M 개)에서 **핵심 지식 추출** 후, 인스트럭션 튜닝 데이터 생성
- **인스트럭션 복잡도 조절 기법(Instruction Evolution Method)** 적용 → 난이도별 학습 데이터 구축

![image.png](attachment:f69df2b8-1296-49cd-b776-02ab9de6be28:image.png)

### **2️⃣ 인간 선호 최적화 (Preference Optimization)**

- **DPO(Direct Preference Optimization), SimPO** 등 최신 기법 활용
- **최적의 응답(y_w)과 최악의 응답(y_l) 선택**하여 모델이 더 인간 친화적인 답변을 생성하도록 학습
- 단계적 훈련 방식 적용 (M0 → M1 → M2)하여 **과적합 방지**

![image.png](attachment:7bb84f2e-eb68-4337-b5e5-ab269d7310e5:image.png)

---

## 🔹 **긴 문맥 처리 성능**

EXAONE 3.5는 **긴 문맥 내에서 정보 검색 및 이해** 능력이 뛰어남.

### **1️⃣ Needle-in-a-Haystack (NIAH) 테스트**

- 긴 문서 내에서 **특정 정보를 찾는 능력** 평가
- EXAONE 3.5는 **32K 토큰 범위 내에서 거의 완벽한 검색 성능** 달성
- 한국어와 영어 모두에서 **우수한 성능 기록**

### **2️⃣ LongBench, LongRAG 테스트**

- 긴 문맥 이해 벤치마크(📖 LongBench, 📚 LongRAG)에서도 경쟁 모델 대비 우수한 성능
- **Ko-WebRAG 구축**: 한국어 웹 검색 기반 QA 성능까지 검증

---

## 🔹 **결론**

EXAONE 3.5는 **"실제 환경에서 활용 가능한 강력한 LLM"** 으로 개발됨.

✅ **32K 토큰의 긴 문맥을 효과적으로 처리**

✅ **비슷한 크기의 모델보다 적은 연산량으로 높은 성능**

✅ **데이터 정제, 지도 학습, 인간 선호 최적화 등 최신 기법 적용**

✅ **한국어와 영어에서 강력한 성능**

---

### 📌 **이 논문의 핵심 기여점**

1. **긴 문맥 처리 능력 개선** (32K 토큰, 망각 문제 해결)
2. **비용 대비 효율적인 학습** (적은 연산량으로 동급 모델과 경쟁)
3. **한국어 및 영어 모두 강력한 성능 제공**
4. **다양한 크기의 모델 제공 (2.4B ~ 32B) → 실제 환경에서 활용 가능**
5. **엄격한 데이터 정제 및 인공지능 윤리 준수**

이 논문이 제안하는 **EXAONE 3.5는 한국어를 포함한 다국어에서 강력한 성능을 보이며, 실제 서비스에서 활용할 수 있는 실용적인 LLM** 입니다. 💡
